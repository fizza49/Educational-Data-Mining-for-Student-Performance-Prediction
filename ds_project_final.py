# -*- coding: utf-8 -*-
"""DS_Project final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jXMqbM0ChfDhiH6iJxE1v4HEugk-nXoo

# **Educational Data Mining for Student Performance Prediction**

**Loading Dataset**
"""

import pandas as pd
import numpy as np
data = pd.read_csv("/content/Student_performance_data _.csv")

# displaying first five rows
data.head()

print("Shape:", data.shape)

print("Data types:\n", data.dtypes)

print("Missing values:\n", data.isnull().sum())

print("\nSummary stats:\n", data.describe())

"""# **Data Cleaning**

**Handling Missing Values**

There was no missing values in the dataset.

**Check for impossible values**
"""

# Valid GPA range (GPA can't be less than 0 or greater than 4)
data = data[(data['GPA'] >= 0) & (data['GPA'] <= 4)]

# Minimum reasonable age (university students cant be less than 15 years old)
data = data[data['Age'] >= 15]

"""**Remove duplicates**"""

data.drop_duplicates(inplace=True)

"""# **Saving cleaned dataset**"""

data.to_csv('Cleaned_Student_Performance_Data.csv', index=False)

"""# **Exploratory Data Analysis (EDA) for Educational Data Mining: Student Performance Prediction**

### **Import Required Libraries**
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Make plots look nicer
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (10, 5)

"""## **1. Load the Dataset**"""

# Load CSV
df = pd.read_csv("/content/Cleaned_Student_Performance_Data.csv")

# Display first few rows
df.head()

"""## **2. Understand the Structure**

**Shape of the Dataset**
"""

print("Shape of the dataset:", df.shape)

"""**Column Names and Data Types**"""

print("Column names and data types:\n")
print(df.dtypes)

"""**Sample Rows**"""

print("First 5 rows:")
print(df.head())

print("\nLast 5 rows:")
print(df.tail())

"""**Basic Information**"""

# Display info about columns, non-null counts, and data types
print("\nDataset Info:")
print(df.info())

# Print unique values for all columns
for col in df.columns:
    print(f"\nColumn: {col}")
    print("Unique values:", df[col].unique())

categorical_cols = ['Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'Tutoring',
                    'ParentalSupport', 'Extracurricular', 'Sports', 'Music',
                    'Volunteering', 'GradeClass']

# Convert to category dtype
df[categorical_cols] = df[categorical_cols].astype('category')

# Verify
print(df.dtypes)

"""##**3. Missing Values Analysis**"""

# Count of missing values per column
print("Missing values per column:")
print(df.isnull().sum())

# Percentage of missing values
print("\nPercentage of missing values:")
print((df.isnull().sum() / len(df)) * 100)

"""**Check for Duplicates And Remove duplicates**"""

# Check for duplicate rows
print("Duplicate Rows Count:", df.duplicated().sum())

data.drop_duplicates(inplace=True)

"""##**4. Summary Statistics**"""

# Summary statistics for numerical columns
print("Descriptive statistics:")
print(df.describe())

# Check for object columns before calling describe
object_cols = df.select_dtypes(include=['object']).columns
if len(object_cols) > 0:
    print(df[object_cols].describe())
else:
    print("No categorical columns found in the DataFrame.")

"""**For Categorical Features**"""

# Value counts for encoded categorical columns
print("\nValue counts for Gender:")
print(df['Gender'].value_counts())
print("\nValue counts for GradeClass:")
print(df['GradeClass'].value_counts())

"""##**5. Univariate Analysis**"""

# Distribution of GPA
sns.histplot(df['GPA'], kde=True, bins=30)
plt.title("Distribution of GPA")
plt.xlabel("GPA")
plt.ylabel("Frequency")
plt.show()

# Boxplot of Study Time per Week
sns.boxplot(y='StudyTimeWeekly', data=df)
plt.title("Boxplot: Weekly Study Time")
plt.ylabel("Hours")
plt.show()

# Count plot of Grade Classes
sns.countplot(x='GradeClass', data=df)
plt.title("Count of Each Grade Class")
plt.xlabel("Grade Class")
plt.ylabel("Number of Students")
plt.show()

"""### **Data Distribution**

**Histograms**
"""

# Select numerical columns
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Calculate layout dimensions dynamically
num_cols = len(numerical_columns)
nrows = 2
ncols = (num_cols + nrows - 1) // nrows  # Ensure enough columns

# Plot histograms for each numerical feature with adjusted layout
df[numerical_columns].hist(bins=15, figsize=(15, 8), layout=(nrows, ncols))
plt.suptitle("Histograms of Numerical Features")
plt.tight_layout()
plt.show()

"""**Boxplots**"""

# Boxplots to detect outliers in numerical columns
for col in numerical_columns:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot of {col}")
    plt.show()

"""### **Categorical Feature Analysis**

**Count Plots**
"""

# Plot count plots for each categorical column
for col in categorical_cols:
    plt.figure(figsize=(8, 4))
    sns.countplot(data=df, x=col)
    plt.title(f"Count Plot of {col}")
    plt.xticks(rotation=45)
    plt.show()

"""**Pie Charts**"""

# Pie charts to visualize category distribution
for col in categorical_cols:
    plt.figure(figsize=(5, 5))
    df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)
    plt.title(f"Pie Chart of {col}")
    plt.ylabel("")  # Remove y-axis label
    plt.show()

"""##**6. Bivariate Analysis**"""

# GPA vs Study Time
sns.scatterplot(x='StudyTimeWeekly', y='GPA', data=df)
plt.title("GPA vs Study Time Weekly")
plt.xlabel("Study Time Weekly")
plt.ylabel("GPA")
plt.show()

# GPA by Gender
sns.boxplot(x='Gender', y='GPA', data=df)
plt.title("GPA by Gender")
plt.xlabel("Gender (0 = Male, 1 = Female)")
plt.ylabel("GPA")
plt.show()

# GPA by Parental Education Level
sns.boxplot(x='ParentalEducation', y='GPA', data=df)
plt.title("GPA by Parental Education")
plt.xlabel("Parental Education (Encoded)")
plt.ylabel("GPA")
plt.show()

"""## **7. Correlation Analysis**"""

# Heatmap of correlations between numeric variables
plt.figure(figsize=(12, 8))
correlation_matrix = df.corr(numeric_only=True)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

"""## **8. Covariance Analysis**"""

# Compute the covariance matrix
covariance_matrix = df.cov(numeric_only=True)

# Display the covariance matrix
print("\nCovariance Matrix:")
print(covariance_matrix)

# Visualize the covariance matrix (not as common, but informative)
plt.figure(figsize=(12, 8))
sns.heatmap(covariance_matrix, annot=True, cmap='Blues')
plt.title("Covariance Heatmap")
plt.show()

"""**9. Categorical Cross-tabulation**"""

# Crosstab between Gender and GradeClass
cross_tab = pd.crosstab(df['Gender'], df['GradeClass'])
print("\nCrosstab: Gender vs GradeClass")
print(cross_tab)

"""## **10.Automated EDA Tool**

Downloading ydata-profiling
"""

!pip install ydata-profiling

from ydata_profiling import ProfileReport

# Generate and display profile report
profile = ProfileReport(df, title="ðŸ“˜ Student Performance Data Report", explorative=True)
profile.to_notebook_iframe()

"""# **Feature Engineering**

**Creating new features**
"""

data['TotalExtracurricular'] = data['Sports'] + data['Music'] + data['Volunteering']

data['StudyEfficiency'] = data['GPA'] / (data['StudyTimeWeekly'] + 1)

data['AttendanceRate'] = 1 - (data['Absences'] / 30)

"""**Binning continuous variables**"""

'''data['StudyTimeCategory'] = pd.cut(
    data['StudyTimeWeekly'],
    bins=[0, 5, 15, 25, np.inf],
    labels=['Low', 'Medium', 'High', 'Very High']
)'''

"""**One-hot encoding for categorical variables**"""

data = pd.get_dummies(data, columns=['Gender', 'Ethnicity', 'ParentalEducation'])

"""# **Outlier Detection & Handling**

**Detecting outliers using IQR**
"""

def handle_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data[column] = np.where(
        data[column] < lower_bound, lower_bound,
        np.where(data[column] > upper_bound, upper_bound, data[column]))
    return data

"""**Applying to numerical columns**"""

numerical_cols = ['StudyTimeWeekly', 'Absences', 'GPA']
for col in numerical_cols:
    data = handle_outliers(data, col)

"""# **Data Splitting & Scaling**

**Importing Libraries**
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""**Defining features (X) and target (y)**"""

X = data.drop(['GPA', 'GradeClass'], axis=1)
y = data['GPA']

"""**Splitting into train/test sets**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Scaling numerical features**"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['float64', 'int64']))
X_test_scaled = scaler.transform(X_test.select_dtypes(include=['float64', 'int64']))

"""# **Dataset Export**

**Saving train/test sets**
"""

pd.DataFrame(X_train_scaled).to_csv('X_train.csv', index=False)
pd.DataFrame(X_test_scaled).to_csv('X_test.csv', index=False)
y_train.to_csv('y_train.csv', index=False)
y_test.to_csv('y_test.csv', index=False)

"""# **Exploratory Data Analysis (EDA) for Educational Data Mining: Student Performance Prediction**

## **1. Load the Dataset**
"""

# Load CSV
df = pd.read_csv("/content/Cleaned_Student_Performance_Data.csv")

# Display first few rows
df.head()

"""## **2. Understand the Structure**

**Shape of the Dataset**
"""

print("Shape of the dataset:", df.shape)

"""**Column Names and Data Types**"""

print("Column names and data types:\n")
print(df.dtypes)

"""**Sample Rows**"""

print("First 5 rows:")
print(df.head())

print("\nLast 5 rows:")
print(df.tail())

"""**Basic Information**"""

df.info()

"""##**3. Missing Values Analysis**"""

# Count of missing values per column
print("Missing values per column:")
print(df.isnull().sum())

# Percentage of missing values
print("\nPercentage of missing values:")
print((df.isnull().sum() / len(df)) * 100)

"""**Visualizing Missing Values**

**Check for Duplicates And Remove duplicates**
"""

df.duplicated().sum()

data.drop_duplicates(inplace=True)

"""# **Model Training**

# **Linear Regression:**
"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder

model = LinearRegression()
# Train (fit) the model
model.fit(X_train , y_train)

y_pred = model.predict(X_test)

# Compare predicted vs actual values
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
comparison

mse = np.mean((y_test - y_pred) ** 2)
mae = np.mean(np.abs(y_test - y_pred))
rmse_test = np.sqrt(mse)
ss_total = np.sum((y_test - np.mean(y_test)) ** 2)
ss_residual = np.sum((y_test - y_pred) ** 2)
r2 = 1 - (ss_residual / ss_total)

print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"RMSE: {rmse_test}")
print(f"RÂ² Score: {r2}")

"""# **Random Forest Regressor**"""

from sklearn.ensemble import RandomForestRegressor

X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

# Align columns after encoding (important!)
X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)

# Initialize and train the model
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

mse = np.mean((y_test - y_pred) ** 2)
mae = np.mean(np.abs(y_test - y_pred))
rmse_test = np.sqrt(mse)
ss_total = np.sum((y_test - np.mean(y_test)) ** 2)
ss_residual = np.sum((y_test - y_pred) ** 2)
r2 = 1 - (ss_residual / ss_total)

print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"RMSE: {rmse_test}")
print(f"RÂ² Score: {r2}")

# Compare predicted vs actual values
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
comparison

"""# **Decision Tree Regressor**"""

from sklearn.tree import DecisionTreeRegressor

# Initialize the regressor
regressor = DecisionTreeRegressor(max_depth=5, random_state=42)

# Train the model
regressor.fit(X_train, y_train)

# Make predictions
y_pred = regressor.predict(X_test)

mse = np.mean((y_test - y_pred) ** 2)
mae = np.mean(np.abs(y_test - y_pred))
rmse_test = np.sqrt(mse)
ss_total = np.sum((y_test - np.mean(y_test)) ** 2)
ss_residual = np.sum((y_test - y_pred) ** 2)
r2 = 1 - (ss_residual / ss_total)

print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"RMSE: {rmse_test}")
print(f"RÂ² Score: {r2}")

# Compare predicted vs actual values
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
comparison

"""# **K Nearest Neighbor Regression**"""

from sklearn.neighbors import KNeighborsRegressor

# KNN regressor
knn = KNeighborsRegressor(n_neighbors=3)
knn.fit(X_train_scaled, y_train)

# Predict GPA
y_pred = knn.predict(X_test_scaled)

mse = np.mean((y_test - y_pred) ** 2)
mae = np.mean(np.abs(y_test - y_pred))
rmse_test = np.sqrt(mse)
ss_total = np.sum((y_test - np.mean(y_test)) ** 2)
ss_residual = np.sum((y_test - y_pred) ** 2)
r2 = 1 - (ss_residual / ss_total)

print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"RMSE: {rmse_test}")
print(f"RÂ² Score: {r2}")

# Compare predicted vs actual values
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
comparison

"""# **Ensemble Model**"""

from sklearn.ensemble import  VotingRegressor

# Create ensemble model
ensemble = VotingRegressor(estimators=[
    ('knn', knn),
    ('model', model),
    ('rf', rf),
    ('regressor', regressor)
])

# Fit ensemble model
ensemble.fit(X_train, y_train)

# Predict and evaluate
y_pred = ensemble.predict(X_test)

mse = np.mean((y_test - y_pred) ** 2)
mae = np.mean(np.abs(y_test - y_pred))
rmse_test = np.sqrt(mse)
ss_total = np.sum((y_test - np.mean(y_test)) ** 2)
ss_residual = np.sum((y_test - y_pred) ** 2)
r2 = 1 - (ss_residual / ss_total)

print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"RMSE: {rmse_test}")
print(f"RÂ² Score: {r2}")

# Compare predicted vs actual values
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
comparison

from sklearn.ensemble import StackingRegressor

#base models
base_models = [
    ('rf', rf),
    ('knn', knn),
    ('regressor', regressor),
    ('model',model)
]

# Meta-model
meta_model = LinearRegression()

# Stacking ensemble
stacked_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)

# Train
stacked_model.fit(X_train, y_train)

# Predict and evaluate
y_pred = stacked_model.predict(X_test)

mse = np.mean((y_test - y_pred) ** 2)
mae = np.mean(np.abs(y_test - y_pred))
rmse_test = np.sqrt(mse)
ss_total = np.sum((y_test - np.mean(y_test)) ** 2)
ss_residual = np.sum((y_test - y_pred) ** 2)
r2 = 1 - (ss_residual / ss_total)

print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"RMSE: {rmse_test}")
print(f"RÂ² Score: {r2}")

# Compare predicted vs actual values
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
comparison

import matplotlib.pyplot as plt

# Model names
model_names = [
    'Linear Regression',
    'Random Forest',
    'Decision Tree',
    'KNN',
    'Voting Regressor',
    'Stacked Regressor'
]

# Actual evaluation metrics
mae_scores = [0.0385, 0.0304, 0.0976, 0.1285, 0.0555, 0.0243]
mse_scores = [0.1547, 0.1204, 0.2456, 0.2864, 0.1836, 0.1150]
rmse_scores = [0.1963, 0.1744, 0.3124, 0.3586, 0.2357, 0.1562]

# Plotting
x = range(len(model_names))
plt.figure(figsize=(12, 6))

plt.plot(x, mae_scores, label='MAE', marker='o', color='blue')
plt.plot(x, mse_scores, label='MSE', marker='s', color='green')
plt.plot(x, rmse_scores, label='RMSE', marker='^', color='red')

plt.xticks(x, model_names, rotation=45, ha="right")
plt.xlabel("Models")
plt.ylabel("Error")
plt.title("Model Performance Comparison (MAE, MSE, RMSE)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Model names
model_names = [
    'Linear Regression',
    'Random Forest',
    'Decision Tree',
    'KNN',
    'Voting Regressor',
    'Stacked Regressor'
]

# RÂ² scores
r2_scores = [0.9533, 0.9631, 0.8819, 0.8445, 0.9327, 0.9705]

# Plot
plt.figure(figsize=(10, 5))
bars = plt.bar(model_names, r2_scores, color='mediumseagreen')

# Add value labels on top of bars
for bar in bars:
    height = bar.get_height()
    plt.annotate(f'{height:.4f}',
                 xy=(bar.get_x() + bar.get_width() / 2, height),
                 xytext=(0, 5),
                 textcoords="offset points",
                 ha='center', va='bottom')

plt.title('RÂ² Score Comparison of Models')
plt.xlabel('Models')
plt.ylabel('RÂ² Score')
plt.ylim(0.8, 1.0)
plt.xticks(rotation=45, ha="right")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""# **Predicting the Grade Class**

0: 'A' (GPA >= 3.5)

1: 'B' (3.0 <= GPA < 3.5)

2: 'C' (2.5 <= GPA < 3.0)

3: 'D' (2.0 <= GPA < 2.5)

4: 'F' (GPA < 2.0)
"""

#Importing necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

X=data.drop(['GradeClass'],axis=1)
y=data['GradeClass']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""# **Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

from sklearn.metrics import precision_score, recall_score, f1_score

# Single averaged scores
precision = precision_score(y_test, y_pred_rf, average='macro')  # or 'weighted'
recall = recall_score(y_test, y_pred_rf, average='macro')
f1 = f1_score(y_test, y_pred_rf, average='macro')

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Compare predicted vs actual values
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_rf})
comparison

"""# **XgBoost Classifier**"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['float64', 'int64']))
X_test_scaled = scaler.transform(X_test.select_dtypes(include=['float64', 'int64']))

import xgboost as xgb

# Create and train XGBoost model
xgb_model = xgb.XGBClassifier(objective='multi:softmax',
                              num_class=5,                # Number of classes
                              eval_metric='mlogloss',
                              random_state=42)

xgb_model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred_xgb = xgb_model.predict(X_test_scaled)

print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))

from sklearn.metrics import precision_score, recall_score, f1_score

# Single averaged scores
precision = precision_score(y_test, y_pred_xgb, average='macro')  # or 'weighted'
recall = recall_score(y_test, y_pred_xgb, average='macro')
f1 = f1_score(y_test, y_pred_xgb, average='macro')

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

models = ['Random Forest', 'XGBoost']
precision = [0.8928, 0.8676]
recall = [0.8402, 0.8564]
f1_score = [0.8607, 0.8606]

x = range(len(models))

width = 0.2

plt.bar(x, precision, width, label='Precision')
plt.bar([i + width for i in x], recall, width, label='Recall')
plt.bar([i + 2 * width for i in x], f1_score, width, label='F1-score')

plt.xticks([i + width for i in x], models)  # Center x-axis ticks
plt.ylabel('Score')
plt.title('Precision, Recall, and F1-score Comparison')
plt.legend()
plt.show()

!pip install -q streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import seaborn as sns
# import matplotlib.pyplot as plt
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# 
# st.set_page_config(page_title="Student Performance App", layout="wide")
# st.title("Student Performance Analysis App")
# 
# # File uploader
# uploaded_file = st.file_uploader("Upload CSV file", type=["csv"], key="upload1")
# 
# if uploaded_file is not None:
#     # Read data
#     data = pd.read_csv(uploaded_file)
#     st.subheader("Raw Dataset")
#     st.dataframe(data)
# 
#     # Data shape and info
#     st.write("### Dataset Info")
#     st.write(f"Rows: {data.shape[0]}, Columns: {data.shape[1]}")
# 
#     if st.checkbox("Show column types"):
#         st.write(data.dtypes)
# 
#     # Preprocessing
#     st.write("### Preprocessing")
# 
#     # Handle missing values (if any)
#     st.write("Handling missing values (if any)...")
#     data.fillna(data.mode().iloc[0], inplace=True)
# 
#     # Encode categorical variables
#     st.write("Encoding categorical features...")
#     label_encoders = {}
#     for column in data.select_dtypes(include=['object']).columns:
#         le = LabelEncoder()
#         data[column] = le.fit_transform(data[column])
#         label_encoders[column] = le
# 
#     st.success("Preprocessing complete.")
# 
#     # Exploratory Data Analysis
#     st.write("### Exploratory Data Analysis")
#     if st.checkbox("Show correlation heatmap"):
#         fig, ax = plt.subplots(figsize=(10, 6))
#         sns.heatmap(data.corr(), annot=True, cmap='coolwarm', ax=ax)
#         st.pyplot(fig)
# 
#     if st.checkbox("Show GPA distribution"):
#         fig, ax = plt.subplots()
#         sns.histplot(data['GPA'], kde=True, ax=ax)
#         st.pyplot(fig)
# 
#     if st.checkbox("Show boxplot for GPA"):
#         fig, ax = plt.subplots()
#         sns.boxplot(y=data['GPA'], ax=ax)
#         st.pyplot(fig)
# 
#     # Split data
#     st.write("### Splitting Data")
#     X = data.drop('GPA', axis=1)
#     y = data['GPA']
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 
#     # Train model
#     model = RandomForestRegressor(random_state=42)
#     model.fit(X_train, y_train)
#     y_pred = model.predict(X_test)
# 
#     # Model evaluation
#     mse = mean_squared_error(y_test, y_pred)
#     mae = mean_absolute_error(y_test, y_pred)
#     r2 = r2_score(y_test, y_pred)
#     st.write("### Model Evaluation")
#     st.metric(label="Mean Squared Error", value=f"{mse:.2f}")
#     st.metric(label="Mean Absolute Error", value=f"{mae:.2f}")
#     st.metric(label="RÂ² Score", value=f"{r2:.2f}")
# 
#     # Feature importance
#     st.write("### Feature Importance")
#     importances = model.feature_importances_
#     features = X.columns
#     indices = np.argsort(importances)[::-1]
# 
#     fig, ax = plt.subplots(figsize=(10, 5))
#     ax.bar(range(X.shape[1]), importances[indices])
#     ax.set_xticks(range(X.shape[1]))
#     ax.set_xticklabels(features[indices], rotation=45, ha='right')
#     ax.set_title("Feature Importances")
#     st.pyplot(fig)
# 
#     # Prediction form
#     st.write("### Predict GPA from Input Features")
#     input_data = {}
#     for col in X.columns:
#         if col in label_encoders:
#             options = label_encoders[col].classes_.tolist()
#             selected = st.selectbox(f"{col}", options)
#             input_data[col] = label_encoders[col].transform([selected])[0]
#         else:
#             input_data[col] = st.number_input(f"{col}", value=float(X[col].mean()))
# 
#     if st.button("Predict GPA"):
#         input_df = pd.DataFrame([input_data])
#         prediction = model.predict(input_df)[0]
#         st.success(f"Predicted GPA: {prediction:.2f}")
# else:
#     st.info("Please upload a CSV file to get started.")

from pyngrok import conf, ngrok
import subprocess
import time

# Set ngrok auth token
conf.get_default().auth_token = "2wdRq4wuDt4uKvzZ63pqzGCryD0_4PX8Yw9RejwCu2CPqvmcZ"

# Kill any existing ngrok tunnels
ngrok.kill()

# Start Streamlit app as a subprocess
subprocess.Popen(["streamlit", "run", "app.py", "--server.port=8501"])
time.sleep(3)

# Start ngrok tunnel
try:
    public_url = ngrok.connect(addr="8501", proto="http")
    print("Streamlit app running at:", public_url)
except Exception as e:
    print(f"Error starting ngrok: {e}")